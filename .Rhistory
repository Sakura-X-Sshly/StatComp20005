}
p=mean(abs(c(t0, reps))>= abs(t0))
p
}
# calculate the t1e
tle1=rep(0,200)
for(i in 1:200) tle1[i]=ptf(rnorm(20,0,1),rnorm(30,0,1))
t1e=mean(tle1<=0.05)
t1e
# calculate the power
pow1=rep(0,200)
for(i in 1:200) pow1[i]=ptf(rnorm(20,0,1),rnorm(30,0,2))
power=mean(pow1<=0.05)
power
set.seed(1)
library(RANN)
library(boot)
library(Ball)
library(energy)
m=1e2; k=3; p=2; n1=n2=20
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]
n2 <- sizes[2]
n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1)
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
R<-999; N = c(n1,n2)
# Unequal variances and equal expectations
for(i in 1:m){
x <- matrix(rnorm(n1*p),ncol=p);
y <-matrix(rnorm(n2*p,0,2),ncol=p)
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value # performance of NN method
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value # performance of energy method
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value # performance of ball method
}
alpha <- 0.05                        # confidence level
pow <- colMeans(p.values<alpha)
print(pow)
# Unequal variances and unequal expectations
for(i in 1:m){
x <- matrix(rnorm(n1*p),ncol=p);
y <-cbind(rnorm(n2,0,2),rnorm(n2,1,1))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05
pow <- colMeans(p.values<alpha)
print(pow)
# Non-normal distributions: t distribution with 1 df (heavy-taileddistribution),
for(i in 1:m){
x <- matrix(rt(n1*p,1),ncol=p);
y <-cbind(rnorm(n2,0,1),rnorm(n2,0,1))
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05
pow <- colMeans(p.values<alpha)
print(pow)
# bimodel distribution (mixture of two normal distributions)
n=n1+n2
for(i in 1:m){
x <-matrix(rnorm(n2*p),ncol=p)
y1 <- rnorm(n1*p)
y2 <- rnorm(n1*p,1,2)
w  <- rbinom(n, 1, .5) ;
y <- matrix(w*y1 + (1-w)*y2,ncol=p);
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05
pow <- colMeans(p.values<alpha)
print(pow)
# Unbalanced samples (say, 1 case versus 10 controls)
n1<-10;n2<-100;N=c(n1,n2)
for(i in 1:m){
x <- matrix(rt(n1*p,1),ncol=p);
y <- matrix(rnorm(n2*p,1,2),ncol=p);
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.05
pow <- colMeans(p.values<alpha)
print(pow)
# density function of the standard Laplace distribution
fsld=function(x) 0.5*exp(-abs(x))
# implement a random walk Metropolis sampler
rw.Metropolis=function(sigma,x0,N) {
x=numeric(N)
x[1]=x0
u=runif(N)
k=0 # record the number of candidate points rejected
for (i in 2:N) {
y=rnorm(1,x[i-1],sigma)
if (u[i]<= (fsld(y)/fsld(x[i-1])))
x[i]=y # accept y
else {
x[i]=x[i-1]
k=k+1
} }
return(list(x=x,k=k))
}
set.seed(12345)
N=2000 # length of each chain
sigma=c(0.05, 0.5, 2, 16) # different variances
x0=0 # initialize x0
rw=matrix(c(rep(0,N*4)),nrow=4)
aprate=c(rep(0,4))
for (i in 1:4) { rw[i,]=rw.Metropolis(sigma[i], x0, N)$x # derive each chain
aprate[i]=1-rw.Metropolis(sigma[i], x0, N)$k/N # compute the acceptance rates of each chain
}
# Compare the chains generated when different variances
par(mfrow=c(2,2))
plot(rw[i,],xlab='sigma=0.05',ylim=c(-4,4),ylab="X")
abline(h=-log(0.05), lty=2) # abline 95% quantile
abline(h=log(0.05), lty=2)
plot(rw[i,],xlab='sigma=0.5',ylab="X")
abline(h=-log(0.05), lty=2) # abline 95% quantile
abline(h=log(0.05), lty=2)
plot(rw[i,],xlab='sigma=2',ylab="X")
abline(h=-log(0.05), lty=2) # abline 95% quantile
abline(h=log(0.05), lty=2)
plot(rw[i,],xlab='sigma=16',ylab="X")
abline(h=-log(0.05), lty=2) # abline 95% quantile
abline(h=log(0.05), lty=2)
par(mfrow=c(1,1))
# print the acceptance rates of each chain
print(rbind(sigma,aprate))
# plot the hist and true density
par(mfrow=c(1,2))
hist(rw[2,], breaks=seq(min(rw[2,])-1,max(rw[2,])+1, by=0.1), main="hist of the second chain", freq=FALSE)
x=seq(-8,8,0.1)
y=fsld(x)
lines(x,y,col="red",lwd=3)
# Q-Q plot
a=ppoints(100)
QR=c(log(2*a[a<=0.5]),-log(2*(1-a[a>0.5]))) # quantiles of Laplace
Q=quantile(rw[2,], a)
qqplot(QR, Q, main="qqplot of the second chain",
xlab="Laplace Quantiles", ylab="Sample Quantiles")
lines(c(min(rw[2,])-1,max(rw[2,])+1),c(min(rw[2,])-1,max(rw[2,])+1))
par(mfrow=c(1,1))
# density function of the standard Laplace distribution
fsld=function(x) 0.5*exp(-abs(x))
# the function for computing the mean of each chain
Gelman.Rubin=function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi=as.matrix(psi)
n=ncol(psi)
k=nrow(psi)
psi.means=rowMeans(psi) # row means
B=n*var(psi.means) # between variance est.
psi.w=apply(psi, 1, "var") # within variances
W=mean(psi.w) # within est.
v.hat=W*(n-1)/n+(B/n) # upper variance est.
r.hat=v.hat/W # G-R statistic
return(r.hat)
}
# implement a random walk Metropolis sampler
rw.Metropolis=function(sigma,x0,N) {
x=numeric(N)
x[1]=x0
u=runif(N)
for (i in 2:N) {
y=rnorm(1,x[i-1],sigma)
if (u[i]<= (fsld(y)/fsld(x[i-1])))
x[i]=y # accept y
else x[i]=x[i-1]
}
return(x)
}
# set parameters
set.seed(12345)
k=4 # number of chains
sigma=0.5
sigma1=2
n=12000 # length of each chain
b=1000 # burn-in length
x0=c(-10,-5,5,10) # initialize x0
# sigma=0.5
# generate the chains
X=matrix(0, nrow=2*k, ncol=n)
for (i in 1:k) X[i,]=rw.Metropolis(sigma,x0[i],n)
# compute diagnostic statistics
psi=t(apply(X,1,cumsum))
for (i in 1:nrow(psi))
psi[i,]=psi[i,]/(1:ncol(psi))
print(Gelman.Rubin(psi))
# plot psi for the four chains
par(mfrow=c(2,2))
for (i in 1:k)
plot(psi[i, (b+1):n], type="l",
xlab=i, ylab=bquote(psi))
par(mfrow=c(1,1))
# plot the sequence of R-hat statistics
rhat= rep(0, n)
for (j in (b+1):n)
rhat[j]= Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="sigma=0.5", ylab="R")
abline(h=1.2, lty=2)
Find=function(k) {
# function S_{k-1}(a)
S1= function (a) {
1-pt(sqrt(a^2*(k-1)/(k-a^2)),df=k-1)
}
# function S_{k}(a)
S2= function (a) {
1-pt(sqrt(a^2*k/(k+1-a^2)),df = k)
}
# function S_{k}(a)-S_{k-1}(a)
S=function (a) {
S2(a)-S1(a)
}
# Find the intersection points A(k) in (0,sqrt(k))
eps=.Machine$double.eps^0.5
return(uniroot(S,interval = c(eps, 2-eps))$root)
}
# print the results
k=c(4:25, 100, 500, 1000)
intersection=sapply(k,function(k){Find(k)})
ips=cbind(intersection,k)
print(ips)
# the log likelihood function of observed data
llof <- function (p, q, nA. = 444, nB. = 132, nAB = 63, nO = 361) {
r = 1.0 - p - q
nA. * log(p^2 + 2*p*r) + nB. * log(q^2 + 2 * q * r) +
nAB * log(2 * p * q) + 2 * nO * log(r)
}
EMf <- function (p, q, nA. = 444, nB. = 132, nAB = 63, nO = 361, output = FALSE) {
iter <- 0
while (iter <= 10)
{
# estimate the frequency of r
r <- 1.0 - p - q
# First we carry out the E-step
# Estimate the counts for the other genotypes
nAA <- nA. * p / (p + 2*r)
nAO <- nA. - nAA
nBB <- nB. * q / (q + 2*r)
nBO <- nB. - nBB
# record the log-likelihood value
llv <- llof(p, q, nA., nB., nAB, nO)
# output relevant information
if (output)
{
cat("iteration #", iter, "llikelihood value= ", llv, "\n")
cat("    Allele frequencies: p = ", p, ", q = ", q, ",r = ", r, "\n")
}
# M-step
# p and q are the maixmum point of the log
# likelihood function of completed data
p <- (2 * nAA + nAO + nAB) / (2 * (nA. + nB. + nO + nAB))
q <- (2 * nBB + nBO + nAB) / (2 * (nA. + nB. + nO + nAB))
iter <- iter + 1
}
list(p = p, q = q)
}
EMf(0.4, 0.2, nA. = 444, nB. = 132, nAB = 63, nO = 361, output = TRUE)
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
# use for lapply()
model1<-lapply(formulas, lm, data=mtcars)
model1
# use for loop
model2 <- vector("list", length(formulas))
for (i in seq_along(formulas)) {
model2[[i]] <- lm( formulas[[i]], mtcars)
}
model2
devtools::build_vignettes()
devtools::document()
devtools::build_vignettes()
ADMM <- function(x, y, lambda, n, theta, gamma, epsilon){
# the smoothly clipped absolute deviation penalty (SCAD) is used in this ADMM algorithm
# define ST function
###--------ST--------###
positive <- function(x){
if(x >= 0) x <- x
else x <- 0
return(x)
}
ST <- function(t, lambda){
return(sign(t)*positive(abs(t)-lambda))
}
###--------step 1--------###
# define relevant matrixs
# these list are used to record iterations in each step
r <- list()
mu <- list()
nu <- list()
eta <- list()
beta <- list()
delte <- list()
I <- diag(n)
delta <- matrix(0,n*(n-1)/2,n)
for(i in 1:(n-1)) {
for(j in (i+1):n)  { if(i==1) delta[j-i,] <- I[,i]-I[,j]
else    delta[(2*n-i)*(i-1)/2+j-i,] <- I[,i]-I[,j] }
}
beta[[1]] <- solve(t(x)%*%x)%*%t(x)%*%y
mu[[1]] <- y - x%*%beta[[1]]
r[[1]] <- c(rep(0,n*(n-1)/2))
nu[[1]] <- c(rep(0,n*(n-1)/2))
eta[[1]] <- c(rep(0,n*(n-1)/2))
delte[[1]] <- c(rep(0,n*(n-1)/2))
k <- 1
for(i in 1:n){
for(j in 1:n){
if(i<j){
eta[[1]][k] <- mu[[1]][i] - mu[[1]][j]
k <- k+1
}
}
}
r[[1]] <- delta%*%mu[[1]] - eta[[1]]
###--------step 2&3--------###
m <- 1 # record iteration steps
Qx <- x%*%solve(t(x)%*%x)%*%t(x)
# detailed iteration process
while(m==1||sum(r[[m]]^2) > epsilon){
# update mu and beta
mu[[m+1]] <- solve(theta*t(delta)%*%delta+I-Qx)%*%((I-Qx)%*%y+theta*t(delta)%*%(eta[[m]]-theta^(-1)*nu[[m]]))
beta[[m+1]] <- solve(t(x)%*%x)%*%t(x)%*%(y - mu[[m+1]])
k <- 1
delte[[m+1]] <- c(rep(0,n*(n-1)/2))
eta[[m+1]] <- c(rep(0,n*(n-1)/2))
# update eta
for(i in 1:n){
for(j in 1:n){
if(i < j){
delte[[m+1]][k] <- mu[[m+1]][i] - mu[[m+1]][j] + theta^(-1)*nu[[m]][k]
a <- abs(delte[[m+1]][k])
if(a<=(lambda+lambda/theta)){
eta[[m+1]][k] <- ST(delte[[m+1]][k],lambda/theta)
}
if(a>(lambda+lambda/theta) & a<=(gamma*lambda)){
eta[[m+1]][k] <- ST(delte[[m+1]][k],gamma*lambda/((gamma-1)*theta))/(1-1/((gamma-1)*theta))
}
if(a>(gamma*lambda)){eta[[m+1]][k] <- delte[[m+1]][k]}
k <- k+1
}
}
}
k <- 1
# update nu
nu[[m+1]] <- c(rep(0,n*(n-1)/2))
for(i in 1:n){
for(j in 1:n){
if(i < j){
nu[[m+1]][k] <- nu[[m]][k]+theta*(mu[[m+1]][i]-mu[[m+1]][j]-eta[[m+1]][k])
k <- k+1
}
}
}
r[[m+1]] <- delta%*%mu[[m+1]] - eta[[m+1]]
m <- m+1
}
# the estimators are recorded in a list
return(list(mu_hat = mu[[m]],beta_hat = beta[[m]],eta_hat=eta[[m]]))
}
mean_imputation <- function(y0, n, per) {
# the logarithm of the complete survival time data
# the size of sample
# the censoring rate
T <- exp(y0)
T1 <- T
a <- 1
k <- 1:n
while(a<=n*per)  {   ran1 <- runif(1, 0, sort(T)[0.95*n])
ran2 <- sample(k,1,replace=TRUE)
if(ran2==0) next;
if(ran1<T1[ran2]&&k[ran2]==ran2) { a <- a + 1
k[ran2] <- 0
T1[ran2] <- ran1
}
}
sdelta <- c(rep(1,n)) # the censoring indicator
for(i in 1:n) if(T1[i]<T[i]) sdelta[i] <- 0
C <- T1[which(sdelta==0)]  # observed right censoring times
C1 <- T1[which(sdelta==1)]  # observed true failure times
if(max(T1)==max(C)) { C1 <- c(C1,max(C))
C <- C[which(C!=max(C))]
sdelta[which(T1==max(T1))] <- 1 }
# take the biggest observed time as true failure time
tau <- sort(unique(C))  # different censoring times
tau1 <- sort(unique(C1)) # different failure times
len <- length(tau)
Nc <- c(rep(0,len))
R <- c(rep(0,len))
for(i in 1:len) { R[i] <- length(which(C==tau[i]))
Nc[i] <- length(which(T1>=tau[i]))-length(which(C1==tau[i])) }
len1 <- length(tau1)
Nc1 <- c(rep(0,len1))
R1 <- c(rep(0,len1))
for(i in 1:len1) { R1[i] <- length(which(C1==tau1[i]))
Nc1[i] <- length(which(T1>=tau1[i])) }
# define Sc function
# the K-M estimator of the survival function of the right censoring variable
Sc <- function(x) { g <- which(tau<=x)
if(length(g)==0) return(1)
else return(prod(1-R[1:max(g)]/Nc[1:max(g)])) }
# define S function
# the K-M estimator of the survival function of the true failure time variable
S <- function(x) { g <- which(tau1<=x)
if(length(g)==0) return(1)
else return(prod(1-R1[1:max(g)]/Nc1[1:max(g)])) }
tau2 <- c(0,tau1)
triS <- c(rep(0,len1))
for(i in 1:len1) triS[i] <- S(tau2[i])-S(tau2[i+1])
# modify the observed survival times by the mean imputation method
y <- matrix(0,n,1)
for(i in 1:n) { if(sdelta[i]==1) y[i] <- y0[i]
else y[i] <- sum(log(tau1[which(tau1>T1[i])])*triS[which(tau1>T1[i])])/S(T1[i]) }
return(list(y_hat=y))
}
parking_solution <- function(x, n) {
# x: the parking interval
# the number of simulations
set.seed(123)
p1 <- function(a=0, b=x, c=0){
while(TRUE){
if(b-a<1) return(c)
t<- runif(1,a,b)    # give the location of the first car
if(t+1>b) next
else {c <- c+1 ; break}
}
a1 <- a
a2 <- t+1
b1 <- t
b2 <- b
c <- p1(a1,b1,c)
c <- p1(a2,b2,c)
return(c)
}
d <- numeric()
i <- 0
while(i<=n){
d <- c(d,p1(0,x,0))
i <- i+1
}
m1 <- mean(d)
lim1 <- m1/x
return(list(M_x=m1, lim_M_x=lim1))
}
parking_solution(1000,1000)
devtools::check()
devtools::document()
rm(list = c("ADMM", "mean_imputation", "parking_solution"))
devtools::document()
devtools::build_vignettes()
.Last.error.trace
devtools::check()
devtools::check()
Sys.which(Sys.getenv("R_QPDF","qpdf"))
devtools::document()
warnings()
devtools::build_vignettes()
devtools::check()
Sys.setenv('_R_CHECK_SYSTEM_CLOCK_' = 0
)
devtools::check()
devtools::document()
devtools::document()
devtools::document()
warnings()
devtools::document()
warnings()
rm(list = c("ADMM", "mean_imputation", "parking_solution"))
devtools::document()
warnings()
devtools::document()
devtools::document()
warnings()
devtools::document()
devtools::build_vignettes()
devtools::check()
devtools::document()
devtools::build_vignettes()
devtools::check()
devtools::check()
Sys.setenv('_R_CHECK_SYSTEM_CLOCK_' = 0
)
devtools::check()
devtools::build()
install.packages('../StatComp20005_1.0.zip',repo=NULL)
